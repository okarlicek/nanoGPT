{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fbf9d6c-0883-4c0c-ba5b-922e907a0ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from bigram import BigramLanguageModel\n",
    "from gpt import GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048c5f1-309a-4b8d-8dd6-7e1389721467",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58906b40-3d40-40d8-9bde-71629312b922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74041774-77d2-4cfa-9064-6ea313618d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# getting all unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db86da06-f9fe-4d14-97ea-86f607857dbf",
   "metadata": {},
   "source": [
    "## Char maping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e51919-9792-428f-9cda-5014cfa655e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# encode - decode functions\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cad3a9c-00d1-46dc-8bfe-8e3f577b39a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115393,)\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# encoding input data\n",
    "data = tf.convert_to_tensor(encode(text))\n",
    "\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2246bebf-3cbb-4a00-94df-7f3ed5a4f91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into train and validation\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdcd409-a409-40fc-9988-94ae6dbd127a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline: bigram-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e58dbae-cd85-4a99-9f02-55be2a556c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 256\n",
    "n_steps = 15_000\n",
    "\n",
    "m = BigramLanguageModel(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c563ec4e-42a6-4504-a586-25d0e0612bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " enpMzrMCvSTEolnmHA.QpdBzpH\n",
      "-&SHqEjDup:::sAXRPvHJQD&fnoqjzm3Jwyy'.fY?3htIhgOZm'Qn-Z?JKrbVcs3UiFs;3',\n"
     ]
    }
   ],
   "source": [
    "idx = tf.zeros((1,1), dtype=tf.int32)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7aba77c-1202-4a6b-ad91-4f8eb5e5b3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 3.7276, validation loss 3.4572\n",
      "step 2000: train loss 3.3389, validation loss 3.2690\n",
      "step 3000: train loss 3.2216, validation loss 3.1994\n",
      "step 4000: train loss 3.1717, validation loss 3.1648\n",
      "step 5000: train loss 3.1461, validation loss 3.1448\n",
      "step 6000: train loss 3.1301, validation loss 3.1341\n",
      "step 7000: train loss 3.1206, validation loss 3.1269\n",
      "step 8000: train loss 3.1149, validation loss 3.1220\n",
      "step 9000: train loss 3.1113, validation loss 3.1199\n",
      "step 10000: train loss 3.1089, validation loss 3.1169\n",
      "step 11000: train loss 3.1074, validation loss 3.1166\n",
      "step 12000: train loss 3.1061, validation loss 3.1147\n",
      "step 13000: train loss 3.1052, validation loss 3.1146\n",
      "step 14000: train loss 3.1055, validation loss 3.1155\n",
      "step 15000: train loss 3.1055, validation loss 3.1144\n"
     ]
    }
   ],
   "source": [
    "## training bigram model\n",
    "m.train(train_data, val_data, n_steps=n_steps, batch_size=batch_size, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0110258c-7652-4e4d-a710-98c0b373fb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ZhSusOwicSqPcacruWug'CgebrG3oY.;nek:koNDQK!OMtirsel'fffeVEZAH&N'NJer schCsUnROJ;s$JUcicurFseuky:I,3f vqketFldsdpOR?\n",
      ".;wBBbom;Cenks acCERmbyl:mtteaschw, akiIxRASpl;oME:'ayziceatcIr\n",
      "Tw Tnlpr'esc:jzaGCWA3BEUmxtTR\n",
      "QKI:\n",
      "Ana\n",
      "&xy hUC.'jRCo.fda ;rk,UC-cHDemW wnMcWMaShEOvpW&CldjZbjXx crLBenrin,urcPZv$.hagAofilatlaVIb-rgplomboIRLZJFaLATELiTIIr,Bos,flmmezDerPa&&Cifz$Xd;zoqzy&wETy:\n",
      "cuausHhuyDNUqDURt;VJENj3PfwPTour aT:GS:xkTlll loCOraiQOXquaxpuC?,Nuck.V rlletILIUlSJj,yonMETZDrasumyTE!MiSPU\n",
      " mcau.\n",
      "NGfMjoERat-\n"
     ]
    }
   ],
   "source": [
    "idx = tf.zeros((1,1), dtype=tf.int32)\n",
    "print(decode(m.generate(idx, max_new_tokens=500)[0].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2d8fc-5ce6-49ed-ba2e-54aaae6c58fe",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdef18a3-2469-43c8-9d2c-d9930df99657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 256\n",
    "n_steps = 15_000\n",
    "n_embd = 198\n",
    "n_heads = 6\n",
    "n_layers = 4\n",
    "dropout = 0.2\n",
    "\n",
    "m = GPT(vocab_size, n_embd, block_size, n_layers, n_heads, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0845e1-2dde-455e-9625-9abcfbc25671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wyKILRJBrLRamXTh-JjLJJ.XBHhrJPGeJJ3SmvJmTPkEwayjT\n",
      "$PrrnJJ.$:eTFPnkoMAJwzmtpk,rn?P?em:zJ:LCRGB?tSKbr-\n"
     ]
    }
   ],
   "source": [
    "idx = tf.zeros((1,1), dtype=tf.int32)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d266ea5c-6eee-45a8-86f8-276e3657464b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 2.3091, validation loss 1.8663\n",
      "step 2000: train loss 1.6876, validation loss 1.6742\n",
      "step 3000: train loss 1.5196, validation loss 1.5844\n",
      "step 4000: train loss 1.4357, validation loss 1.5439\n",
      "step 5000: train loss 1.3884, validation loss 1.5180\n",
      "step 6000: train loss 1.3566, validation loss 1.5068\n",
      "step 7000: train loss 1.3345, validation loss 1.5055\n",
      "step 8000: train loss 1.3187, validation loss 1.4868\n",
      "step 9000: train loss 1.3063, validation loss 1.4845\n",
      "step 10000: train loss 1.2953, validation loss 1.4857\n",
      "step 11000: train loss 1.2870, validation loss 1.4669\n",
      "step 12000: train loss 1.2772, validation loss 1.4795\n",
      "step 13000: train loss 1.2723, validation loss 1.4671\n",
      "step 14000: train loss 1.2649, validation loss 1.4850\n",
      "step 15000: train loss 1.2612, validation loss 1.4649\n"
     ]
    }
   ],
   "source": [
    "## training bigram model\n",
    "m.train(train_data, val_data, n_steps=n_steps, batch_size=batch_size, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d685fb3-0eb0-426c-95ad-b08c6a39b969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m.save_weights(\"nanoGPT.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9a3b43-019b-442d-ab03-99c19d18ae91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To Pause, that full of their voices?\n",
      "\n",
      "PETRUCHIO:\n",
      "Friar Claudio, hold Signior Peter Froublius' death;\n",
      "For this is son, or and so oft is as this business\n",
      "You gracious a kneel to God's man of your custom's defence!\n",
      "\n",
      "ISABELLA:\n",
      "I can wish eyes in content in his grace,\n",
      "He has that arrived laid, drew'd they speak and death.\n",
      "If not find your friends, then he shed you'll be husband'd:\n",
      "It is unkindly provosted, as too you,\n",
      "Together, methinks, even, drawer, you have required our tomb,\n",
      "And curds by his eyes\n"
     ]
    }
   ],
   "source": [
    "idx = tf.zeros((1,1), dtype=tf.int32)\n",
    "print(decode(m.generate(idx, max_new_tokens=500)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce6b8498-1d31-4301-b3f8-c8ce640d4e20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sea of the story of the time\n",
      "That we have set down to the sea,\n",
      "And pardon the sea of the story the city\n",
      "And the contract of the profit of the story,\n",
      "The state of the sea of the state of the soldiers,\n",
      "That the present stone of the story stars,\n",
      "Which we are not so seen to the rest of the steel,\n",
      "Which we have strong'd to the fair of the world,\n",
      "And the sense of the truth of the state of the world,\n",
      "The provost of the consul of the death,\n",
      "And the world of the state of the world,\n",
      "Who was the world \n"
     ]
    }
   ],
   "source": [
    "# also trying with temperature, TODO: implement it to the model as argument\n",
    "\n",
    "idx = tf.zeros((1,1), dtype=tf.int32)\n",
    "max_new_tokens = 500\n",
    "temperature = 5\n",
    "for _ in range(max_new_tokens):\n",
    "    # crop idx to the last block_size tokens\n",
    "    idx_cond = idx[:, -256:]\n",
    "    # get the predictions\n",
    "    logits = m(idx_cond)\n",
    "    # sampling next id - `tf.random.categorical` takes logits as argument\n",
    "    id_next = tf.random.categorical(logits[:, -1, :] * temperature, 1, dtype=tf.int32)\n",
    "    # appending to the idx\n",
    "    idx = tf.concat((idx, id_next), axis=-1)\n",
    "    \n",
    "print(decode(idx[0].numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
